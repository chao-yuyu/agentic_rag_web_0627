{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 多格式RAG AI Agent 已就緒 ===\n",
      "現在支援: TXT, PDF, MD, JSON, DOCX, DOC, XLSX, XLS\n",
      "輸入 'quit' 退出程式\n",
      "輸入 'list' 查看已載入的文檔\n",
      "\n",
      "開始處理問題: 找有關BE200發生過的issue\n",
      "1. 搜索相關文檔...\n",
      "找到 4 個候選文檔\n",
      "找到的文檔:\n",
      "  1. 11.txt (txt) - 相似度: 0.667\n",
      "  2. 15.txt (txt) - 相似度: 0.601\n",
      "  3. 494.txt (txt) - 相似度: 0.588\n",
      "  4. 157.txt (txt) - 相似度: 0.587\n",
      "\n",
      "2. 篩選相關文檔...\n",
      "\u001b[33mfilter_user_proxy\u001b[0m (to DocumentFilter):\n",
      "\n",
      "\n",
      "用戶問題: 找有關BE200發生過的issue\n",
      "\n",
      "文檔內容:\n",
      "編號與日期: (11) 20241014\n",
      "客戶: TW Jetone\n",
      "購買產品: IMB-X1314\n",
      "問題描述：\n",
      "TW Jetone 公司反映其使用的 IMB-X1314 搭配 Intel BE200 在清除 CMOS 或 BIOS 預設載入後，裝置管理員無法偵測該裝置。進一步測試顯示，在 S3、S4、S5 狀態下無法正常重置 BE200，導致裝置無法被系統辨識。\n",
      "\n",
      "解決辦法：\n",
      "經技術支援部門（TSD）和研發部門（RD）確認，問題源於早期 ADL-S 主板的 PCHU1 接 +3V 造成 S3/S4/S5 時無法正常拉低 BUF_PLT_RST#，進而影響 BE200 的重置。建議的解決方案是將 PCHU1 改接至 +3VSB，並通過 AND 閘控制 M2/PCIe 插槽的 BUF_PLT_RST#。目前，建議客戶若需要 WiFi 7 功能，可以先更換 Qualcomm NCM865 模組作為替代方案。未來 PCB 改版時將一併導入此修正。\n",
      "\n",
      "專有名詞解釋：\n",
      "- IMB-X1314：東擎科技生產的一款工業級主板。\n",
      "- Intel BE200：Intel 的一款整合 WiFi 7 和 Bluetooth 的 M.2 模組。\n",
      "- BUF_PLT_RST#：一種信號線，用於控制系統重置。\n",
      "- PCHU1：主板上的特定連接點，用於供電控制。\n",
      "- +3VSB：待機電壓，即使系統處於待機模式也能提供電力。\n",
      "\n",
      "請判斷此文檔是否與用戶問題相關。\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 06-10 09:48:48] {696} WARNING - Model qwen3:14b is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mDocumentFilter\u001b[0m (to filter_user_proxy):\n",
      "\n",
      "<think>\n",
      "好的，我需要仔细分析用户的问题和文档内容，看看是否符合筛选标准。用户的问题是关于BE200发生的issue，而文档中的问题描述提到了IMB-X1314搭配Intel BE200在清除CMOS或BIOS默认加载后，设备管理器无法检测该设备，并且在S3、S4、S5状态下无法正常重置BE200，导致装置无法被系统识别。解决办法部分也提到了这个问题的原因和解决方案。\n",
      "\n",
      "首先，产品型号方面，用户提到的BE200在文档中确实有出现，作为Intel的M.2模块。不过用户的问题可能是指东擎科技的产品，但文档中的BE200是Intel的，而东擎科技的主板是IMB-X1314。不过用户的问题只是询问有关BE200发生的issue，而文档中的问题确实涉及BE200与IMB-X1314的兼容性问题，所以产品型号部分可能符合，因为BE200是问题中的关键组件。\n",
      "\n",
      "其次，客户方面，用户没有指定客户，文档中的客户是TW Jetone，但用户没有提到客户，所以这点不影响相关性。\n",
      "\n",
      "问题类型相关方面，文档中的问题确实与BE200相关，具体是BE200在特定情况下无法被检测到，这与用户查询的BE200发生的issue直接相关。因此，问题类型是相关的。\n",
      "\n",
      "不过需要确认产品型号是否符合。用户的问题可能是指东擎科技的产品型号，但文档中的BE200是Intel的模块，而东擎科技的主板是IMB-X1314。但用户的问题只是关于BE200的issue，而文档中的问题确实涉及BE200，所以可能符合。不过根据筛选标准中的产品型号必须一致，但用户的问题可能没有指定东擎科技的型号，只是BE200。因此，文档中的产品型号是IMB-X1314和BE200，而用户的问题是关于BE200的，所以可能相关。\n",
      "\n",
      "综合来看，文档中的问题确实涉及BE200的issue，因此应该判断为相关。\n",
      "</think>\n",
      "\n",
      "RELEVANT: 文檔中明確提及Intel BE200與IMB-X1314主板搭配使用時發生的問題（清除CMOS後無法被偵測、S3/S4/S5狀態重置異常），且問題核心與BE200的硬件兼容性直接相關，符合用戶查詢的「BE200發生過的issue」需求。\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (c9e69e16-ed57-4a39-9a9f-d59cfbbdc257): Maximum turns (1) reached\u001b[0m\n",
      "\u001b[33mfilter_user_proxy\u001b[0m (to DocumentFilter):\n",
      "\n",
      "\n",
      "用戶問題: 找有關BE200發生過的issue\n",
      "\n",
      "文檔內容:\n",
      "編號與日期: (15) 20240911\n",
      "客戶: UK BVM\n",
      "購買產品: SBC-260\n",
      "問題描述：\n",
      "UK BVM 公司使用的 SBC-260 主機板在安裝 Telit MV32 模組時，系統無法正確辨識該設備，設備管理員中顯示黃色警示符號後消失。初步測試顯示，問題可能與 M.2 模組的時脈信號控制有關，且疑似設備本身存在問題。\n",
      "\n",
      "解決辦法：\n",
      "經過多次測試和 BIOS 更新，最終確定將 M.2 pin 68 設定為 GPIO output high 可以解決問題。ASRock Industrial 提供了 BIOS 版本 P1.10X 和 P1.102，其中 P1.102 包含更深入的 GPIO 重置功能，以確保系統穩定性。客戶在使用 P1.102 BIOS 後，決定採用其他 5G 模組作為替代方案，目前問題已解決，案件結案。\n",
      "\n",
      "專有名詞解釋：\n",
      "- SBC-260：ASRock Industrial 生產的一款單板電腦（Single Board Computer）。\n",
      "- M.2：一種通用的內存和儲存介面規格，常見於現代電腦系統中。\n",
      "- Telit MV32：Telit 公司生產的一種 5G 模組，用於提供無線通信功能。\n",
      "- BIOS：基本輸入輸出系統（Basic Input Output System），是電腦啟動時的第一個執行程式，負責初始化硬體並載入作業系統。\n",
      "\n",
      "請判斷此文檔是否與用戶問題相關。\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 06-10 09:48:54] {696} WARNING - Model qwen3:14b is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mDocumentFilter\u001b[0m (to filter_user_proxy):\n",
      "\n",
      "<think>\n",
      "好的，我需要判断用户的问题是否与提供的文档相关。用户的问题是关于BE200发生过的issue，而文档中的产品型号是SBC-260，客户是UK BVM，问题涉及SBC-260主板在安装Telit MV32模块时的兼容性问题。\n",
      "\n",
      "首先，根据筛选标准第一条，产品型号必须一致。用户提到的BE200与文档中的SBC-260明显不同，虽然都是ASRock Industrial的产品，但型号不匹配。BE200可能属于其他系列，比如4x4-7XXX系列，但文档中的SBC-260属于不同的产品线，没有系列号的对应关系。\n",
      "\n",
      "其次，客户方面，用户没有指定客户，但文档中的客户是UK BVM，这可能不影响相关性，但产品型号不匹配是关键因素。\n",
      "\n",
      "第三，问题类型相关性。用户询问的是BE200的issue，而文档中的问题是关于SBC-260的，虽然都是硬件兼容性问题，但涉及的型号不同，因此不直接相关。\n",
      "\n",
      "综上，产品型号不一致，且没有系列的对应，因此文档不相关。\n",
      "</think>\n",
      "\n",
      "NOT_RELEVANT: 產品型號不一致，用戶詢問的是BE200，但文檔內容是關於SBC-260的問題。\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (2986027d-6529-4512-9a3c-c2e2bdd59ed9): Maximum turns (1) reached\u001b[0m\n",
      "\u001b[33mfilter_user_proxy\u001b[0m (to DocumentFilter):\n",
      "\n",
      "\n",
      "用戶問題: 找有關BE200發生過的issue\n",
      "\n",
      "文檔內容:\n",
      "編號與日期: (494) 20200921\n",
      "客戶: TSD internal\n",
      "購買產品: GX-BEACON\n",
      "問題描述：\n",
      "客戶在使用東擎科技的GX-BEACON產品時，遇到了多項問題，包括LED焊接不良、USB接口多錫、按鍵不良、FW/OS版本錯誤等。這些問題影響了產品的正常使用和品質控制。\n",
      "\n",
      "解決辦法：\n",
      "對於LED焊接不良和USB接口多錫問題，建議客戶將不良品退回工廠進行進一步分析和維修。按鍵不良問題初步分析為按鍵上殘膠導致，已建議廠商改善製程並尋找第二供應源。對於FW/OS版本錯誤，建議客戶更新至最新版本。此外，已建議工廠導入離線燒錄程序，以避免版本錯誤問題。目前，廠商正在進行改善措施，預計新的改善批次將於1月25日生產，並進行庫存換貨。\n",
      "\n",
      "專有名詞解釋：\n",
      "- GX-BEACON：東擎科技生產的一款工業級產品，可能包含特定的硬體配置和功能，用於特定的工業應用場景。\n",
      "- RMA：Return Merchandise Authorization，退貨授權，是一種退貨流程，允許客戶將不良品退回給供應商進行維修或更換。\n",
      "- ECN：Engineering Change Notice，工程變更通知，用於記錄產品設計或製造過程中的變更。\n",
      "\n",
      "請判斷此文檔是否與用戶問題相關。\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 06-10 09:49:01] {696} WARNING - Model qwen3:14b is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mDocumentFilter\u001b[0m (to filter_user_proxy):\n",
      "\n",
      "<think>\n",
      "好的，我需要判断用户的问题是否与提供的文档内容相关。用户的问题是关于BE200发生过的issue，而文档中的产品型号是GX-BEACON。首先，根据型号识别规则，BE200可能属于某个系列，但文档中的产品是GX-BEACON，这看起来是不同的型号。不过，用户提到的BE200可能和文档中的GX-BEACON是否有关系呢？比如，是否属于同一产品线或系列？\n",
      "\n",
      "接下来，检查文档中的客户信息。文档中的客户是TSD internal，而用户没有指定客户，所以客户部分可能匹配，但不确定是否有影响。问题类型方面，文档中提到的LED焊接不良、USB接口多锡、按键不良、FW/OS版本错误等问题，这些是否与用户查询的BE200的issue相关？\n",
      "\n",
      "关键点在于产品型号是否一致。用户询问的是BE200，而文档中是GX-BEACON，这两个型号明显不同，没有系列上的关联。根据型号识别规则，BE200可能属于某个系列，但文档中的产品型号不同，所以不符合产品型号一致的要求。此外，文档中提到的GX-BEACON的问题可能与BE200无关，除非有额外信息表明它们是同一产品，但根据现有信息，没有这样的关联。\n",
      "\n",
      "因此，文档中的产品型号与用户查询的BE200不一致，且问题类型虽然都是产品问题，但型号不同导致不相关。\n",
      "</think>\n",
      "\n",
      "NOT_RELEVANT: 產品型號不一致，文檔中提及的是GX-BEACON，而用戶查詢的是BE200，兩者型號不同且無系列關聯性。\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (e070dc5b-6eee-40ff-854e-d8268fc7d9e6): Maximum turns (1) reached\u001b[0m\n",
      "\u001b[33mfilter_user_proxy\u001b[0m (to DocumentFilter):\n",
      "\n",
      "\n",
      "用戶問題: 找有關BE200發生過的issue\n",
      "\n",
      "文檔內容:\n",
      "編號與日期: (157) 20230522\n",
      "客戶: China 創博威\n",
      "購買產品: IMB-1220\n",
      "問題描述：\n",
      "- 客戶收到的100片IMB-1220主板中有約40片存在來料不良問題，其中15片BLT_VOL1連接器損壞，1片(F6GMTP006533)無法開機。所有主板均存在不同程度的Clear CMOS Header、4-Pin Chassis FAN和System Panel Header針腳歪斜問題。\n",
      "\n",
      "解決辦法：\n",
      "- 已安排將17片較嚴重的主板送往維修中心處理，其餘較輕微問題的主板由客戶自行調整。目前QA正在請工廠提供相關資訊並協助調查不良原因及改善方案。工廠已完成可靠性試驗報告，包括單禮盒震動試驗、整箱下層主板震動試驗及跌落試驗，結果均通過。\n",
      "\n",
      "專有名詞解釋：\n",
      "- IMB-1220：東擎科技(ASRock Industrial)生產的一款工業級主板型號。\n",
      "- BLT_VOL1：主板上的特定連接器名稱。\n",
      "- Clear CMOS Header：用於清除CMOS設置的主板接口。\n",
      "- 4-Pin Chassis FAN：主板上的風扇接口。\n",
      "- System Panel Header：主板上的系統面板接口，通常包含開機按鈕、指示燈等連接。\n",
      "\n",
      "請判斷此文檔是否與用戶問題相關。\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 06-10 09:49:08] {696} WARNING - Model qwen3:14b is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mDocumentFilter\u001b[0m (to filter_user_proxy):\n",
      "\n",
      "<think>\n",
      "好的，我现在需要判断用户的问题是否与提供的文档内容相关。用户的问题是关于BE200发生过的issue，而文档内容涉及的是IMB-1220主板的问题。\n",
      "\n",
      "首先，根据筛选标准第一条，产品型号必须一致。用户提到的BE200和文档中的IMB-1220是不同的型号。不过，可能需要考虑是否有系列上的关联。根据型号识别规则，BE200可能属于某个系列，但文档中的IMB-1220看起来像是一个具体的型号，而不是系列。因此，BE200和IMB-1220可能没有直接的系列关联，所以产品型号不一致。\n",
      "\n",
      "其次，客户方面，用户没有指定客户，文档中的客户是创博威，但用户问题中没有提到客户，所以这点可能不影响，但产品型号不匹配是主要问题。\n",
      "\n",
      "第三，问题类型相关性。文档中描述的是IMB-1220主板的来料不良问题，包括连接器损坏和针脚歪斜等。用户的问题是关于BE200的issue，但BE200的问题未在文档中提及。因此，问题类型虽然都是关于主板的问题，但具体型号不同，所以不相关。\n",
      "\n",
      "综上，产品型号不一致，且文档中没有提到BE200，因此判断为不相关。\n",
      "</think>\n",
      "\n",
      "NOT_RELEVANT: 產品型號不一致，文檔中描述的是IMB-1220主板的問題，而用戶詢問的是BE200相關的issue，兩者型號不同且無系列關聯性。\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (13d227d9-b4b6-4eef-8a08-542f81325e9d): Maximum turns (1) reached\u001b[0m\n",
      "篩選後保留 1 個相關文檔\n",
      "保留的文檔:\n",
      "  1. 11.txt (txt)\n",
      "\n",
      "3. 生成最終答案...\n",
      "\u001b[33msynthesis_user_proxy\u001b[0m (to AnswerSynthesizer):\n",
      "\n",
      "\n",
      "基於以下相關文檔回答用戶問題:\n",
      "\n",
      "用戶問題: 找有關BE200發生過的issue\n",
      "\n",
      "相關文檔:\n",
      "文檔1:\n",
      "編號與日期: (11) 20241014\n",
      "客戶: TW Jetone\n",
      "購買產品: IMB-X1314\n",
      "問題描述：\n",
      "TW Jetone 公司反映其使用的 IMB-X1314 搭配 Intel BE200 在清除 CMOS 或 BIOS 預設載入後，裝置管理員無法偵測該裝置。進一步測試顯示，在 S3、S4、S5 狀態下無法正常重置 BE200，導致裝置無法被系統辨識。\n",
      "\n",
      "解決辦法：\n",
      "經技術支援部門（TSD）和研發部門（RD）確認，問題源於早期 ADL-S 主板的 PCHU1 接 +3V 造成 S3/S4/S5 時無法正常拉低 BUF_PLT_RST#，進而影響 BE200 的重置。建議的解決方案是將 PCHU1 改接至 +3VSB，並通過 AND 閘控制 M2/PCIe 插槽的 BUF_PLT_RST#。目前，建議客戶若需要 WiFi 7 功能，可以先更換 Qualcomm NCM865 模組作為替代方案。未來 PCB 改版時將一併導入此修正。\n",
      "\n",
      "專有名詞解釋：\n",
      "- IMB-X1314：東擎科技生產的一款工業級主板。\n",
      "- Intel BE200：Intel 的一款整合 WiFi 7 和 Bluetooth 的 M.2 模組。\n",
      "- BUF_PLT_RST#：一種信號線，用於控制系統重置。\n",
      "- PCHU1：主板上的特定連接點，用於供電控制。\n",
      "- +3VSB：待機電壓，即使系統處於待機模式也能提供電力。\n",
      "\n",
      "請提供綜合性的答案:\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 06-10 09:49:14] {696} WARNING - Model qwen3:14b is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mAnswerSynthesizer\u001b[0m (to synthesis_user_proxy):\n",
      "\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "根據文檔1的記載，BE200（Intel 的一款整合 WiFi 7 和 Bluetooth 的 M.2 模組）在搭配 IMB-X1314 主板時，曾發生過一個與系統重置相關的問題。具體來說，當用戶在清除 CMOS 或 BIOS 預設載入後，裝置管理員無法偵測到 BE200 裝置。進一步測試發現，在 S3、S4、S5 系統狀態下，BE200 無法正常重置，導致系統無法辨識該裝置。\n",
      "\n",
      "此問題的根源在於早期 ADL-S 主板的 PCHU1 接 +3V 供電設計，導致在 S3/S4/S5 狀態下無法正確拉低 BUF_PLT_RST# 信號線，進而影響 BE200 的重置功能。為解決此問題，技術支援部門（TSD）與研發部門（RD）建議將 PCHU1 改接至 +3VSB（待機電壓），並通過 AND 閘控制 M2/PCIe 插槽的 BUF_PLT_RST# 信號線。\n",
      "\n",
      "目前，若客戶需要 WiFi 7 功能，建議先更換為 Qualcomm NCM865 模組作為替代方案。未來 PCB 改版時將一併導入此修正。\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (e00fffc6-7462-456a-b9d3-d5064d0c8984): Maximum turns (1) reached\u001b[0m\n",
      "\n",
      "答案: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "根據文檔1的記載，BE200（Intel 的一款整合 WiFi 7 和 Bluetooth 的 M.2 模組）在搭配 IMB-X1314 主板時，曾發生過一個與系統重置相關的問題。具體來說，當用戶在清除 CMOS 或 BIOS 預設載入後，裝置管理員無法偵測到 BE200 裝置。進一步測試發現，在 S3、S4、S5 系統狀態下，BE200 無法正常重置，導致系統無法辨識該裝置。\n",
      "\n",
      "此問題的根源在於早期 ADL-S 主板的 PCHU1 接 +3V 供電設計，導致在 S3/S4/S5 狀態下無法正確拉低 BUF_PLT_RST# 信號線，進而影響 BE200 的重置功能。為解決此問題，技術支援部門（TSD）與研發部門（RD）建議將 PCHU1 改接至 +3VSB（待機電壓），並通過 AND 閘控制 M2/PCIe 插槽的 BUF_PLT_RST# 信號線。\n",
      "\n",
      "目前，若客戶需要 WiFi 7 功能，建議先更換為 Qualcomm NCM865 模組作為替代方案。未來 PCB 改版時將一併導入此修正。\n",
      "\n",
      "TERMINATE\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "import ollama\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import uuid\n",
    "\n",
    "# 導入自定義的 JSONVectorDB\n",
    "from vector_db import JSONVectorDB\n",
    "class CustomRAGAgentSystem:\n",
    "    def __init__(self, reset_db=False, db_path=\"./custom_json_rag_db\"):\n",
    "        # Ollama配置\n",
    "        self.base_url = \"http://localhost:11434/v1\"\n",
    "        self.embedding_model = \"tsd_4500datas_summary20250606_epoch11_f32:latest\"\n",
    "        self.llm_model = \"qwen3:14b\"\n",
    "        \n",
    "        # 初始化自定義JSON資料庫\n",
    "        self.db_path = db_path\n",
    "        if reset_db and os.path.exists(db_path):\n",
    "            import shutil\n",
    "            shutil.rmtree(db_path)\n",
    "        \n",
    "        # 創建自定義資料庫實例\n",
    "        self.collection = JSONVectorDB(db_path)\n",
    "        \n",
    "        # 如果需要重置\n",
    "        if reset_db:\n",
    "            try:\n",
    "                self.collection.delete_collection(\"rag_docs\")\n",
    "                print(\"已清除舊的RAG資料庫\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # AutoGen配置\n",
    "        self.config_list = [\n",
    "            {\n",
    "                'base_url': self.base_url,\n",
    "                'api_key': \"fakekey\",\n",
    "                'model': self.llm_model,\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.llm_config = {\n",
    "            \"config_list\": self.config_list,\n",
    "            \"temperature\": 0.1,\n",
    "        }\n",
    "        \n",
    "        self.setup_agents()\n",
    "    \n",
    "    def setup_agents(self):\n",
    "        \"\"\"設置AutoGen代理\"\"\"\n",
    "        \n",
    "        # 文檔篩選代理\n",
    "        self.document_filter = autogen.AssistantAgent(\n",
    "            name=\"DocumentFilter\",\n",
    "            llm_config=self.llm_config,\n",
    "#             system_message=\"\"\"您是文檔篩選專家。您的任務是：\n",
    "# 1. 仔細閱讀提供的文檔內容\n",
    "# 2. 判斷文檔是否與用戶問題相關\n",
    "# 3. 如果相關，返回 \"RELEVANT: [相關原因]\"\n",
    "# 4. 如果不相關，返回 \"NOT_RELEVANT: [不相關原因]\"\n",
    "# 5. 請保持客觀和準確的判斷\n",
    "# 請只返回判斷結果，不要添加其他內容。\"\"\"\n",
    "            system_message=\"\"\"您是東擎科技(ASRock Industrial)技術支援部門(TSD)的專業文檔篩選專家。\n",
    "您的任務是精確判斷歷史技術支援文檔是否與用戶查詢相關。\n",
    "\n",
    "**篩選標準（必須同時滿足）：**\n",
    "1. **產品型號**：文檔中的產品型號必須與用戶詢問的型號一致，但有時候會有系列的問題可以以常理判斷\n",
    "2. **型號識別規則**：\n",
    "   - 4x4-7XXX = 7000系列\n",
    "   - 4x4-6XXX = 6000系列  \n",
    "   - 4x4-5XXX = 5000系列\n",
    "   - 依此類推\n",
    "3. **客戶**：如果用戶指定客戶，文檔中的客戶必須匹配\n",
    "4. **問題類型相關**：文檔中描述的技術問題必須與用戶查詢的問題類型相關\n",
    "5. 如果相關，返回 \"RELEVANT: [相關原因]\"\n",
    "6. 如果不相關，返回 \"NOT_RELEVANT: [不相關原因]\"\n",
    "7. 請保持客觀和準確的判斷\n",
    "請只返回判斷結果，不要添加其他內容。\"\"\"\n",
    "\n",
    "        )\n",
    "        \n",
    "        # 答案整合代理\n",
    "        self.answer_synthesizer = autogen.AssistantAgent(\n",
    "            name=\"AnswerSynthesizer\",\n",
    "            llm_config=self.llm_config,\n",
    "            system_message=\"\"\"您是答案整合專家。您的任務是：\n",
    "1. 基於篩選後的相關文檔，為用戶問題提供綜合性答案\n",
    "2. 整合所有相關信息，提供完整且準確的回答\n",
    "3. 如果信息不足，請明確指出\n",
    "4. 使用繁體中文回答\n",
    "5. 回答完成後說 \"TERMINATE\"\n",
    "請提供詳細、有用的答案。\"\"\"\n",
    "        )\n",
    "    \n",
    "    def add_documents_from_directory(self, directory_path: str, file_patterns: List[str] = None):\n",
    "        \"\"\"從資料夾載入所有文檔到RAG資料庫\"\"\"\n",
    "        if file_patterns is None:\n",
    "            # 新增Word和Excel格式支持\n",
    "            file_patterns = [\n",
    "                \"*.txt\", \"*.pdf\", \"*.md\", \"*.json\",\n",
    "                \"*.docx\", \"*.doc\",  # Word文檔\n",
    "                \"*.xlsx\", \"*.xls\"   # Excel文檔\n",
    "            ]\n",
    "        \n",
    "        print(f\"開始載入資料夾: {directory_path}\")\n",
    "        print(f\"支援的文件格式: {', '.join(file_patterns)}\")\n",
    "        \n",
    "        if not os.path.exists(directory_path):\n",
    "            print(f\"錯誤: 資料夾 {directory_path} 不存在\")\n",
    "            return False\n",
    "        \n",
    "        total_files = 0\n",
    "        successful_files = 0\n",
    "        \n",
    "        # 遍歷所有支援的文件類型\n",
    "        for pattern in file_patterns:\n",
    "            file_path_pattern = os.path.join(directory_path, pattern)\n",
    "            files = glob.glob(file_path_pattern)\n",
    "            \n",
    "            for file_path in files:\n",
    "                total_files += 1\n",
    "                file_ext = os.path.splitext(file_path)[1].lower()\n",
    "                \n",
    "                try:\n",
    "                    print(f\"處理文件: {os.path.basename(file_path)}\")\n",
    "                    \n",
    "                    # 根據文件類型載入內容\n",
    "                    if file_ext == '.pdf':\n",
    "                        success = self.add_document(file_path, doc_type=\"pdf\")\n",
    "                    elif file_ext in ['.txt', '.md']:\n",
    "                        success = self.add_document(file_path, doc_type=\"txt\")\n",
    "                    elif file_ext == '.json':\n",
    "                        success = self.add_json_document(file_path)\n",
    "                    elif file_ext in ['.docx', '.doc']:\n",
    "                        success = self.add_word_document(file_path)\n",
    "                    elif file_ext in ['.xlsx', '.xls']:\n",
    "                        success = self.add_excel_document(file_path)\n",
    "                    else:\n",
    "                        print(f\"跳過不支援的文件類型: {file_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    if success:\n",
    "                        successful_files += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"處理文件 {file_path} 時發生錯誤: {e}\")\n",
    "        \n",
    "        print(f\"\\n載入完成: 成功處理 {successful_files}/{total_files} 個文件\")\n",
    "        return successful_files > 0\n",
    "    \n",
    "    def add_word_document(self, file_path: str):\n",
    "        \"\"\"處理Word文檔 (.docx, .doc)\"\"\"\n",
    "        try:\n",
    "            print(f\"正在處理Word文檔: {file_path}\")\n",
    "            \n",
    "            # 讀取Word文檔\n",
    "            doc = Document(file_path)\n",
    "            \n",
    "            # 提取所有段落文本\n",
    "            full_text = []\n",
    "            for paragraph in doc.paragraphs:\n",
    "                if paragraph.text.strip():  # 忽略空段落\n",
    "                    full_text.append(paragraph.text.strip())\n",
    "            \n",
    "            # 提取表格內容\n",
    "            for table in doc.tables:\n",
    "                table_text = []\n",
    "                for row in table.rows:\n",
    "                    row_text = []\n",
    "                    for cell in row.cells:\n",
    "                        cell_text = cell.text.strip()\n",
    "                        if cell_text:\n",
    "                            row_text.append(cell_text)\n",
    "                    if row_text:\n",
    "                        table_text.append(\" | \".join(row_text))\n",
    "                \n",
    "                if table_text:\n",
    "                    full_text.append(\"表格內容:\\n\" + \"\\n\".join(table_text))\n",
    "            \n",
    "            # 合併所有文本\n",
    "            text = \"\\n\\n\".join(full_text)\n",
    "            \n",
    "            if not text.strip():\n",
    "                print(f\"警告: Word文檔 {file_path} 沒有文本內容\")\n",
    "                return False\n",
    "            \n",
    "            # 文本分割\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=850,\n",
    "                chunk_overlap=100,  # Word文檔可能需要更多重疊\n",
    "                length_function=len,\n",
    "            )\n",
    "            chunks = text_splitter.split_text(text)\n",
    "            \n",
    "            # 準備批量數據\n",
    "            ids = []\n",
    "            embeddings = []\n",
    "            documents = []\n",
    "            metadatas = []\n",
    "            \n",
    "            # 生成嵌入並準備數據\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                response = ollama.embed(model=self.embedding_model, input=chunk)\n",
    "                embedding = response[\"embeddings\"][0]\n",
    "                \n",
    "                doc_id = f\"{os.path.basename(file_path)}_{i}\"\n",
    "                ids.append(doc_id)\n",
    "                embeddings.append(embedding)\n",
    "                documents.append(chunk)\n",
    "                metadatas.append({\n",
    "                    \"source\": file_path, \n",
    "                    \"chunk_id\": i, \n",
    "                    \"file_type\": \"docx\",\n",
    "                    \"paragraphs_count\": len(doc.paragraphs),\n",
    "                    \"tables_count\": len(doc.tables)\n",
    "                })\n",
    "            \n",
    "            # 批量添加到資料庫\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings,\n",
    "                documents=documents,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "            \n",
    "            print(f\"成功添加Word文檔: {file_path}，共{len(chunks)}個片段\")\n",
    "            print(f\"  - 段落數: {len(doc.paragraphs)}\")\n",
    "            print(f\"  - 表格數: {len(doc.tables)}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"添加Word文檔失敗: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def add_excel_document(self, file_path: str):\n",
    "        \"\"\"處理Excel文檔 (.xlsx, .xls)\"\"\"\n",
    "        try:\n",
    "            print(f\"正在處理Excel文檔: {file_path}\")\n",
    "            \n",
    "            # 讀取Excel文件\n",
    "            try:\n",
    "                # 讀取所有工作表\n",
    "                excel_file = pd.ExcelFile(file_path)\n",
    "                sheet_names = excel_file.sheet_names\n",
    "                print(f\"  - 發現 {len(sheet_names)} 個工作表: {sheet_names}\")\n",
    "                \n",
    "                all_text = []\n",
    "                \n",
    "                for sheet_name in sheet_names:\n",
    "                    print(f\"  - 處理工作表: {sheet_name}\")\n",
    "                    \n",
    "                    # 讀取工作表\n",
    "                    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "                    \n",
    "                    if df.empty:\n",
    "                        print(f\"    工作表 {sheet_name} 為空，跳過\")\n",
    "                        continue\n",
    "                    \n",
    "                    # 構建工作表文本\n",
    "                    sheet_text = [f\"工作表: {sheet_name}\"]\n",
    "                    sheet_text.append(f\"行數: {len(df)}, 列數: {len(df.columns)}\")\n",
    "                    sheet_text.append(\"列名: \" + \" | \".join(str(col) for col in df.columns))\n",
    "                    sheet_text.append(\"\")\n",
    "                    \n",
    "                    # 將DataFrame轉換為文本\n",
    "                    # 限制每個工作表最多處理1000行，避免過大\n",
    "                    max_rows = min(1000, len(df))\n",
    "                    for i in range(max_rows):\n",
    "                        row_data = []\n",
    "                        for col in df.columns:\n",
    "                            cell_value = df.iloc[i][col]\n",
    "                            # 處理NaN值\n",
    "                            if pd.isna(cell_value):\n",
    "                                cell_value = \"\"\n",
    "                            else:\n",
    "                                cell_value = str(cell_value).strip()\n",
    "                            row_data.append(cell_value)\n",
    "                        \n",
    "                        if any(row_data):  # 只添加非空行\n",
    "                            sheet_text.append(\" | \".join(row_data))\n",
    "                    \n",
    "                    # 如果有更多行，添加說明\n",
    "                    if len(df) > max_rows:\n",
    "                        sheet_text.append(f\"... (還有 {len(df) - max_rows} 行數據)\")\n",
    "                    \n",
    "                    all_text.append(\"\\n\".join(sheet_text))\n",
    "                \n",
    "                # 合併所有工作表文本\n",
    "                text = \"\\n\\n\" + \"=\"*50 + \"\\n\\n\".join(all_text)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"讀取Excel文件時發生錯誤: {e}\")\n",
    "                return False\n",
    "            \n",
    "            if not text.strip():\n",
    "                print(f\"警告: Excel文檔 {file_path} 沒有數據內容\")\n",
    "                return False\n",
    "            \n",
    "            # 文本分割 - Excel文檔通常比較結構化，使用較大的chunk\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1200,  # Excel數據可以使用更大的chunk\n",
    "                chunk_overlap=100,\n",
    "                length_function=len,\n",
    "            )\n",
    "            chunks = text_splitter.split_text(text)\n",
    "            \n",
    "            # 準備批量數據\n",
    "            ids = []\n",
    "            embeddings = []\n",
    "            documents = []\n",
    "            metadatas = []\n",
    "            \n",
    "            # 生成嵌入並準備數據\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                response = ollama.embed(model=self.embedding_model, input=chunk)\n",
    "                embedding = response[\"embeddings\"][0]\n",
    "                \n",
    "                doc_id = f\"{os.path.basename(file_path)}_{i}\"\n",
    "                ids.append(doc_id)\n",
    "                embeddings.append(embedding)\n",
    "                documents.append(chunk)\n",
    "                metadatas.append({\n",
    "                    \"source\": file_path, \n",
    "                    \"chunk_id\": i, \n",
    "                    \"file_type\": \"excel\",\n",
    "                    \"sheets_count\": len(sheet_names),\n",
    "                    \"sheet_names\": sheet_names\n",
    "                })\n",
    "            \n",
    "            # 批量添加到資料庫\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings,\n",
    "                documents=documents,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "            \n",
    "            print(f\"成功添加Excel文檔: {file_path}，共{len(chunks)}個片段\")\n",
    "            print(f\"  - 工作表數: {len(sheet_names)}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"添加Excel文檔失敗: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def add_json_document(self, file_path: str):\n",
    "        \"\"\"專門處理JSON文檔\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                json_data = json.load(f)\n",
    "            \n",
    "            # 將JSON轉換為文本\n",
    "            if isinstance(json_data, dict):\n",
    "                text = json.dumps(json_data, ensure_ascii=False, indent=2)\n",
    "            elif isinstance(json_data, list):\n",
    "                text = \"\\n\".join([json.dumps(item, ensure_ascii=False) for item in json_data])\n",
    "            else:\n",
    "                text = str(json_data)\n",
    "            \n",
    "            # 文本分割\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=850,\n",
    "                chunk_overlap=0,\n",
    "                length_function=len,\n",
    "            )\n",
    "            chunks = text_splitter.split_text(text)\n",
    "            \n",
    "            # 準備批量數據\n",
    "            ids = []\n",
    "            embeddings = []\n",
    "            documents = []\n",
    "            metadatas = []\n",
    "            \n",
    "            # 生成嵌入並準備數據\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                response = ollama.embed(model=self.embedding_model, input=chunk)\n",
    "                embedding = response[\"embeddings\"][0]  # ollama返回的是列表的列表\n",
    "                \n",
    "                doc_id = f\"{os.path.basename(file_path)}_{i}\"\n",
    "                ids.append(doc_id)\n",
    "                embeddings.append(embedding)\n",
    "                documents.append(chunk)\n",
    "                metadatas.append({\n",
    "                    \"source\": file_path, \n",
    "                    \"chunk_id\": i, \n",
    "                    \"file_type\": \"json\"\n",
    "                })\n",
    "            \n",
    "            # 批量添加到資料庫\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings,\n",
    "                documents=documents,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "            \n",
    "            print(f\"成功添加JSON文檔: {file_path}，共{len(chunks)}個片段\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"添加JSON文檔失敗: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def add_document(self, file_path: str, doc_type: str = \"txt\"):\n",
    "        \"\"\"添加單個文檔到RAG資料庫\"\"\"\n",
    "        try:\n",
    "            # 根據文件類型載入文檔\n",
    "            if doc_type.lower() == \"pdf\":\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                documents = loader.load()\n",
    "                text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "            else:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "            \n",
    "            # 文本分割\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=850,\n",
    "                chunk_overlap=0,\n",
    "                length_function=len,\n",
    "            )\n",
    "            chunks = text_splitter.split_text(text)\n",
    "            \n",
    "            # 準備批量數據\n",
    "            ids = []\n",
    "            embeddings = []\n",
    "            documents = []\n",
    "            metadatas = []\n",
    "            \n",
    "            # 生成嵌入並準備數據\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                response = ollama.embed(model=self.embedding_model, input=chunk)\n",
    "                embedding = response[\"embeddings\"][0]  # ollama返回的是列表的列表\n",
    "                \n",
    "                doc_id = f\"{os.path.basename(file_path)}_{i}\"\n",
    "                ids.append(doc_id)\n",
    "                embeddings.append(embedding)\n",
    "                documents.append(chunk)\n",
    "                metadatas.append({\n",
    "                    \"source\": file_path, \n",
    "                    \"chunk_id\": i, \n",
    "                    \"file_type\": doc_type\n",
    "                })\n",
    "            \n",
    "            # 批量添加到資料庫\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings,\n",
    "                documents=documents,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "            \n",
    "            print(f\"成功添加文檔: {file_path}，共{len(chunks)}個片段\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"添加文檔失敗: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def list_loaded_documents(self):\n",
    "        \"\"\"顯示已載入的文檔列表\"\"\"\n",
    "        try:\n",
    "            all_docs = self.collection.get(include=[\"metadatas\"])\n",
    "            \n",
    "            # 統計不同的文件\n",
    "            file_stats = {}\n",
    "            for metadata in all_docs[\"metadatas\"]:\n",
    "                source = metadata.get(\"source\", \"unknown\")\n",
    "                filename = os.path.basename(source)\n",
    "                file_type = metadata.get(\"file_type\", \"unknown\")\n",
    "                \n",
    "                if filename not in file_stats:\n",
    "                    file_stats[filename] = {\n",
    "                        \"chunks\": 0,\n",
    "                        \"type\": file_type,\n",
    "                        \"path\": source,\n",
    "                        \"extra_info\": {}\n",
    "                    }\n",
    "                file_stats[filename][\"chunks\"] += 1\n",
    "                \n",
    "                # 添加額外信息\n",
    "                if file_type == \"docx\":\n",
    "                    file_stats[filename][\"extra_info\"][\"paragraphs\"] = metadata.get(\"paragraphs_count\", 0)\n",
    "                    file_stats[filename][\"extra_info\"][\"tables\"] = metadata.get(\"tables_count\", 0)\n",
    "                elif file_type == \"excel\":\n",
    "                    file_stats[filename][\"extra_info\"][\"sheets\"] = metadata.get(\"sheets_count\", 0)\n",
    "                    file_stats[filename][\"extra_info\"][\"sheet_names\"] = metadata.get(\"sheet_names\", [])\n",
    "            \n",
    "            print(f\"\\n已載入的文檔列表 (共 {len(file_stats)} 個文件):\")\n",
    "            for filename, stats in file_stats.items():\n",
    "                extra_info = \"\"\n",
    "                if stats[\"type\"] == \"docx\" and stats[\"extra_info\"]:\n",
    "                    extra_info = f\" [段落:{stats['extra_info'].get('paragraphs', 0)}, 表格:{stats['extra_info'].get('tables', 0)}]\"\n",
    "                elif stats[\"type\"] == \"excel\" and stats[\"extra_info\"]:\n",
    "                    sheets = stats['extra_info'].get('sheets', 0)\n",
    "                    extra_info = f\" [工作表:{sheets}]\"\n",
    "                \n",
    "                # 選擇適當的圖標\n",
    "                icon = {\n",
    "                    \"txt\": \"📄\", \"md\": \"📝\", \"pdf\": \"📕\", \n",
    "                    \"json\": \"📋\", \"docx\": \"📘\", \"excel\": \"📊\"\n",
    "                }.get(stats[\"type\"], \"📄\")\n",
    "                \n",
    "                print(f\"  {icon} {filename} ({stats['type']}) - {stats['chunks']} 個片段{extra_info}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"獲取文檔列表失敗: {e}\")\n",
    "    \n",
    "    def search_documents(self, query: str, n_results: int = 4) -> List[Dict]:\n",
    "        \"\"\"搜索最相關的文檔\"\"\"\n",
    "        try:\n",
    "            # 生成查詢嵌入\n",
    "            response = ollama.embed(model=self.embedding_model, input=query)\n",
    "            query_embedding = [response[\"embeddings\"][0]]  # 包裝成二維列表\n",
    "            \n",
    "            # 搜索相似文檔\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=query_embedding,\n",
    "                n_results=n_results\n",
    "            )\n",
    "            \n",
    "            documents = []\n",
    "            for i in range(len(results[\"documents\"][0])):\n",
    "                documents.append({\n",
    "                    \"content\": results[\"documents\"][0][i],\n",
    "                    \"metadata\": results[\"metadatas\"][0][i],\n",
    "                    \"distance\": results[\"distances\"][0][i]\n",
    "                })\n",
    "            \n",
    "            return documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"文檔搜索失敗: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def filter_documents(self, query: str, documents: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"使用第一個LLM篩選文檔\"\"\"\n",
    "        relevant_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            # 為每個文檔篩選創建新的用戶代理\n",
    "            filter_user_proxy = autogen.UserProxyAgent(\n",
    "                name=\"filter_user_proxy\",\n",
    "                human_input_mode=\"NEVER\",\n",
    "                max_consecutive_auto_reply=1,\n",
    "                is_termination_msg=lambda x: True,\n",
    "                code_execution_config=False,\n",
    "                llm_config=self.llm_config,\n",
    "            )\n",
    "            \n",
    "            # 構造篩選提示\n",
    "            filter_prompt = f\"\"\"\n",
    "用戶問題: {query}\n",
    "\n",
    "文檔內容:\n",
    "{doc['content']}\n",
    "\n",
    "請判斷此文檔是否與用戶問題相關。\n",
    "\"\"\"\n",
    "            \n",
    "            # 與文檔篩選代理對話\n",
    "            filter_user_proxy.initiate_chat(\n",
    "                self.document_filter,\n",
    "                message=filter_prompt,\n",
    "                max_turns=1\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                last_message = filter_user_proxy.last_message(self.document_filter)\n",
    "                if last_message and \"NOT_RELEVANT:\" in last_message[\"content\"]:\n",
    "                    continue\n",
    "                else:\n",
    "                    relevant_docs.append(doc)\n",
    "            except Exception as e:\n",
    "                print(f\"獲取篩選結果失敗: {e}\")\n",
    "                # 如果篩選失敗，保留文檔\n",
    "                relevant_docs.append(doc)\n",
    "        \n",
    "        return relevant_docs\n",
    "    \n",
    "    def generate_answer(self, query: str, relevant_docs: List[Dict]) -> str:\n",
    "        \"\"\"使用第二個LLM生成最終答案\"\"\"\n",
    "        if len(relevant_docs) == 0:\n",
    "            return \"沒有找到相關文檔來回答您的問題。\"\n",
    "        \n",
    "        # 為答案生成創建新的用戶代理\n",
    "        synthesis_user_proxy = autogen.UserProxyAgent(\n",
    "            name=\"synthesis_user_proxy\",\n",
    "            human_input_mode=\"NEVER\",\n",
    "            max_consecutive_auto_reply=1,\n",
    "            is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "            code_execution_config=False,\n",
    "            llm_config=self.llm_config,\n",
    "        )\n",
    "        \n",
    "        # 構造整合提示\n",
    "        context = \"\\n\\n\".join([f\"文檔{i+1}:\\n{doc['content']}\" \n",
    "                              for i, doc in enumerate(relevant_docs)])\n",
    "        \n",
    "        synthesis_prompt = f\"\"\"\n",
    "基於以下相關文檔回答用戶問題:\n",
    "\n",
    "用戶問題: {query}\n",
    "\n",
    "相關文檔:\n",
    "{context}\n",
    "\n",
    "請提供綜合性的答案:\n",
    "\"\"\"\n",
    "        \n",
    "        # 與答案整合代理對話\n",
    "        synthesis_user_proxy.initiate_chat(\n",
    "            self.answer_synthesizer,\n",
    "            message=synthesis_prompt,\n",
    "            max_turns=1\n",
    "        )\n",
    "        \n",
    "        # 獲取最終答案\n",
    "        try:\n",
    "            last_message = synthesis_user_proxy.last_message(self.answer_synthesizer)\n",
    "            return last_message[\"content\"] if last_message else \"生成答案失敗\"\n",
    "        except Exception as e:\n",
    "            print(f\"獲取答案失敗: {e}\")\n",
    "            return \"生成答案過程中發生錯誤\"\n",
    "    \n",
    "    def query(self, question: str) -> str:\n",
    "        \"\"\"完整的RAG查詢流程\"\"\"\n",
    "        print(f\"\\n開始處理問題: {question}\")\n",
    "        \n",
    "        # 步驟1: 搜索相關文檔\n",
    "        print(\"1. 搜索相關文檔...\")\n",
    "        documents = self.search_documents(question, n_results=4)\n",
    "        print(f\"找到 {len(documents)} 個候選文檔\")\n",
    "        \n",
    "        if not documents:\n",
    "            return \"沒有找到相關文檔\"\n",
    "        \n",
    "        # 顯示找到的文檔檔案名稱和類型\n",
    "        print(\"找到的文檔:\")\n",
    "        for i, doc in enumerate(documents, 1):\n",
    "            filename = os.path.basename(doc['metadata']['source'])\n",
    "            file_type = doc['metadata'].get('file_type', 'unknown')\n",
    "            distance = doc['distance']\n",
    "            print(f\"  {i}. {filename} ({file_type}) - 相似度: {1-distance:.3f}\")\n",
    "        \n",
    "        # 步驟2: 篩選文檔\n",
    "        print(\"\\n2. 篩選相關文檔...\")\n",
    "        relevant_docs = self.filter_documents(question, documents)\n",
    "        print(f\"篩選後保留 {len(relevant_docs)} 個相關文檔\")\n",
    "        \n",
    "        # 顯示篩選後保留的文檔\n",
    "        if relevant_docs:\n",
    "            print(\"保留的文檔:\")\n",
    "            for i, doc in enumerate(relevant_docs, 1):\n",
    "                filename = os.path.basename(doc['metadata']['source'])\n",
    "                file_type = doc['metadata'].get('file_type', 'unknown')\n",
    "                print(f\"  {i}. {filename} ({file_type})\")\n",
    "        \n",
    "        # 步驟3: 生成答案\n",
    "        print(\"\\n3. 生成最終答案...\")\n",
    "        answer = self.generate_answer(question, relevant_docs)\n",
    "        \n",
    "        return answer\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "def main():\n",
    "    # 初始化自定義RAG系統\n",
    "    rag_system = CustomRAGAgentSystem(\n",
    "        reset_db=False, \n",
    "        db_path=\"./custom_json_rag_db\"\n",
    "    )\n",
    "    \n",
    "    # 從資料夾載入所有文檔（現在支援Word和Excel）\n",
    "    # directory_path = \"/home/asri/TSD_issue/test\"  # 您的文檔資料夾路徑\n",
    "    directory_path = \"/home/asri/TSD_issue/tsd_issue_summary_1140410\"\n",
    "    \n",
    "    # if os.path.exists(directory_path):\n",
    "    #     print(\"=== 從資料夾載入文檔 ===\")\n",
    "    #     rag_system.add_documents_from_directory(\n",
    "    #         directory_path=directory_path,\n",
    "    #         file_patterns=[\n",
    "    #             \"*.txt\", \"*.pdf\", \"*.md\", \"*.json\",\n",
    "    #             \"*.docx\", \"*.doc\",   # Word文檔\n",
    "    #             \"*.xlsx\", \"*.xls\"    # Excel文檔\n",
    "    #         ]\n",
    "    #     )\n",
    "    \n",
    "    # # 顯示已載入的文檔\n",
    "    # rag_system.list_loaded_documents()\n",
    "    \n",
    "    # 互動式查詢\n",
    "    print(\"\\n=== 多格式RAG AI Agent 已就緒 ===\")\n",
    "    print(\"現在支援: TXT, PDF, MD, JSON, DOCX, DOC, XLSX, XLS\")\n",
    "    print(\"輸入 'quit' 退出程式\")\n",
    "    print(\"輸入 'list' 查看已載入的文檔\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\n請輸入您的問題: \")\n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "        elif question.lower() == 'list':\n",
    "            rag_system.list_loaded_documents()\n",
    "            continue\n",
    "        \n",
    "        answer = rag_system.query(question)\n",
    "        print(f\"\\n答案: {answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
