import autogen
import ollama
from typing import List, Dict
import json
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader, PyPDFLoader
import os
import glob
import pandas as pd
from docx import Document
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import uuid
import time
from opencc import OpenCC

# å°å…¥è‡ªå®šç¾©çš„ JSONVectorDB
from vector_db import JSONVectorDB

def convert_to_traditional(text: str) -> str:
    """å°‡ç°¡é«”ä¸­æ–‡è½‰æ›ç‚ºç¹é«”ä¸­æ–‡"""
    cc = OpenCC('s2t')  # ç°¡é«”åˆ°ç¹é«”
    return cc.convert(text)

class CustomRAGAgentSystem:
    def __init__(self, reset_db=False, db_path="./custom_json_rag_db"):
        # Ollamaé…ç½®
        self.base_url = "http://localhost:11434/v1"
        self.embedding_model = "tsd_4500datas_summary20250606_epoch11_f32:latest"
        self.llm_model = "qwen3:30b"
        # self.llm_model = "gemma3:27b"
        
        # åˆå§‹åŒ–è‡ªå®šç¾©è³‡æ–™åº«
        self.db_path = db_path
        if reset_db and os.path.exists(db_path):
            import shutil
            shutil.rmtree(db_path)
        
        # å‰µå»ºè‡ªå®šç¾©è³‡æ–™åº«å¯¦ä¾‹
        self.collection = JSONVectorDB(db_path)
        
        # å¦‚æœéœ€è¦é‡ç½®
        if reset_db:
            try:
                self.collection.delete_collection("rag_docs")
                print("å·²æ¸…é™¤èˆŠçš„RAGè³‡æ–™åº«")
            except:
                pass
        
        # AutoGené…ç½®
        self.config_list = [
            {
                'base_url': self.base_url,
                'api_key': "fakekey",
                'model': self.llm_model,
            }
        ]
        
        self.llm_config = {
            "config_list": self.config_list,
            "temperature": 0.0,
        }
        
        # æ·»åŠ ä»»å‹™ä¸­æ–·æ¨™èªŒ
        self._stop_flag = False
        
        self.setup_agents()
    
    def stop_current_task(self):
        """è¨­ç½®åœæ­¢æ¨™èªŒ"""
        self._stop_flag = True
        print("ä»»å‹™åœæ­¢æ¨™èªŒå·²è¨­ç½®")
    
    def _check_stop_flag(self):
        """æª¢æŸ¥æ˜¯å¦æ‡‰è©²åœæ­¢ä»»å‹™"""
        if self._stop_flag:
            raise Exception("ä»»å‹™å·²è¢«ç”¨æˆ¶ä¸­æ–·")
    
    def setup_agents(self):
        """è¨­ç½®AutoGenä»£ç†"""
        
        # æ–‡æª”ç¯©é¸ä»£ç†
        self.document_filter = autogen.AssistantAgent(
            name="DocumentFilter",
            llm_config=self.llm_config,
#             system_message="""æ‚¨æ˜¯æ–‡æª”ç¯©é¸å°ˆå®¶ã€‚æ‚¨çš„ä»»å‹™æ˜¯ï¼š
# 1. ä»”ç´°é–±è®€æä¾›çš„æ–‡æª”å…§å®¹
# 2. åˆ¤æ–·æ–‡æª”æ˜¯å¦èˆ‡ç”¨æˆ¶å•é¡Œç›¸é—œ
# 3. å¦‚æœç›¸é—œï¼Œè¿”å› "RELEVANT: [ç›¸é—œåŸå› ]"
# 4. å¦‚æœä¸ç›¸é—œï¼Œè¿”å› "NOT_RELEVANT: [ä¸ç›¸é—œåŸå› ]"
# 5. è«‹ä¿æŒå®¢è§€å’Œæº–ç¢ºçš„åˆ¤æ–·
# è«‹åªè¿”å›åˆ¤æ–·çµæœï¼Œä¸è¦æ·»åŠ å…¶ä»–å…§å®¹ã€‚"""
#             system_message="""æ‚¨æ˜¯æ±æ“ç§‘æŠ€(ASRock Industrial)æŠ€è¡“æ”¯æ´éƒ¨é–€(TSD)çš„å°ˆæ¥­æ–‡æª”ç¯©é¸å°ˆå®¶ã€‚æ‚¨çš„ä»»å‹™æ˜¯:
# 1.ä»”ç´°é–±è®€æä¾›çš„æ–‡æª”å…§å®¹
# 2.ç²¾ç¢ºåˆ¤æ–·æ­·å²æŠ€è¡“æ”¯æ´æ–‡æª”æ˜¯å¦èˆ‡ç”¨æˆ¶æŸ¥è©¢ç›¸é—œã€‚
#     **ç¯©é¸æ¨™æº–ï¼ˆå¿…é ˆåŒæ™‚æ»¿è¶³ï¼‰ï¼š**
#     **ç”¢å“å‹è™Ÿ**ï¼šæ–‡æª”ä¸­çš„ç”¢å“å‹è™Ÿå¿…é ˆèˆ‡ç”¨æˆ¶è©¢å•çš„å‹è™Ÿä¸€è‡´ï¼Œä½†æœ‰æ™‚å€™æœƒæœ‰ç³»åˆ—çš„å•é¡Œå¯ä»¥ä»¥å¸¸ç†åˆ¤æ–·
#     **å‹è™Ÿè­˜åˆ¥è¦å‰‡**ï¼š
#     - 4x4-7XXX = 7000ç³»åˆ—
#     - 4x4-6XXX = 6000ç³»åˆ—  
#     - 4x4-5XXX = 5000ç³»åˆ—
#     - ä¾æ­¤é¡æ¨
#     **å®¢æˆ¶**ï¼šæ–‡æª”ä¸­çš„å®¢æˆ¶å¿…é ˆèˆ‡ç”¨æˆ¶æŸ¥è©¢çš„å•é¡ŒåŒ¹é…
#     **å•é¡Œé¡å‹ç›¸é—œ**ï¼šæ–‡æª”ä¸­æè¿°çš„æŠ€è¡“å•é¡Œå¿…é ˆèˆ‡ç”¨æˆ¶æŸ¥è©¢çš„å•é¡Œé¡å‹ç›¸é—œ
# 3. å¦‚æœç›¸é—œï¼Œè¿”å› "RELEVANT: [ç›¸é—œåŸå› ]"
# 4. å¦‚æœä¸ç›¸é—œï¼Œè¿”å› "NOT_RELEVANT: [ä¸ç›¸é—œåŸå› ]"
# 5.è«‹åªè¿”å›åˆ¤æ–·çµæœï¼Œä¸è¦æ·»åŠ å…¶ä»–å…§å®¹ã€‚"""
        system_message="""æ‚¨æ˜¯æ±æ“ç§‘æŠ€(ASRock Industrial)æŠ€è¡“æ”¯æ´éƒ¨é–€(TSD)çš„å°ˆæ¥­æ–‡æª”ç¯©é¸å°ˆå®¶ã€‚æ‚¨çš„ä»»å‹™æ˜¯:
1.ä»”ç´°é–±è®€æä¾›çš„æ–‡æª”å…§å®¹
2.ç²¾ç¢ºåˆ¤æ–·æ­·å²æŠ€è¡“æ”¯æ´æ–‡æª”æ˜¯å¦èˆ‡ç”¨æˆ¶æŸ¥è©¢ç›¸é—œã€‚
3.å®¢æˆ¶å¿…é ˆèˆ‡ç”¨æˆ¶å•é¡Œçš„å®¢æˆ¶åŒ¹é…
4.å¦‚æœç›¸é—œï¼Œè¿”å› "RELEVANT: [ç›¸é—œåŸå› ]"
5.å¦‚æœä¸ç›¸é—œï¼Œè¿”å› "NOT_RELEVANT: [ä¸ç›¸é—œåŸå› ]"
6.è«‹åªè¿”å›åˆ¤æ–·çµæœï¼Œä¸è¦æ·»åŠ å…¶ä»–å…§å®¹ã€‚

**å‹è™Ÿè­˜åˆ¥è¦å‰‡**ï¼š
    - 4x4-7XXX = 7000ç³»åˆ—
    - 4x4-6XXX = 6000ç³»åˆ—  
    - 4x4-5XXX = 5000ç³»åˆ—
    - ä¾æ­¤é¡æ¨
    - NUC MTL = NUC 125 155
"""
        )
        
        # ç­”æ¡ˆæ•´åˆä»£ç†
        self.answer_synthesizer = autogen.AssistantAgent(
            name="AnswerSynthesizer",
            llm_config=self.llm_config,
            system_message="""æ‚¨æ˜¯ç­”æ¡ˆæ•´åˆå°ˆå®¶ã€‚æ‚¨çš„ä»»å‹™æ˜¯ï¼š
1. åŸºæ–¼ç¯©é¸å¾Œçš„ç›¸é—œæ–‡æª”ï¼Œç‚ºç”¨æˆ¶å•é¡Œæä¾›ç¶œåˆæ€§ç­”æ¡ˆ
2. æ•´åˆæ‰€æœ‰ç›¸é—œä¿¡æ¯ï¼Œæä¾›å®Œæ•´ä¸”æº–ç¢ºçš„å›ç­”
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè«‹æ˜ç¢ºæŒ‡å‡º
4. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
è«‹æä¾›è©³ç´°ã€æœ‰ç”¨çš„ç­”æ¡ˆã€‚"""
        )
    
    def add_documents_from_directory(self, directory_path: str, file_patterns: List[str] = None):
        """å¾è³‡æ–™å¤¾è¼‰å…¥æ‰€æœ‰æ–‡æª”åˆ°RAGè³‡æ–™åº«"""
        if file_patterns is None:
            file_patterns = [
                "*.txt", "*.pdf", "*.md", "*.json",
                "*.docx", "*.doc",  # Wordæ–‡æª”
                "*.xlsx", "*.xls"   # Excelæ–‡æª”
            ]
        
        print(f"é–‹å§‹è¼‰å…¥è³‡æ–™å¤¾: {directory_path}")
        print(f"æ”¯æ´çš„æ–‡ä»¶æ ¼å¼: {', '.join(file_patterns)}")
        
        if not os.path.exists(directory_path):
            print(f"éŒ¯èª¤: è³‡æ–™å¤¾ {directory_path} ä¸å­˜åœ¨")
            return False
        
        total_files = 0
        successful_files = 0
        
        for pattern in file_patterns:
            file_path_pattern = os.path.join(directory_path, pattern)
            files = glob.glob(file_path_pattern)
            
            for file_path in files:
                total_files += 1
                file_ext = os.path.splitext(file_path)[1].lower()
                
                try:
                    print(f"è™•ç†æ–‡ä»¶: {os.path.basename(file_path)}")
                    
                    if file_ext == '.pdf':
                        success = self.add_document(file_path, doc_type="pdf")
                    elif file_ext in ['.txt', '.md']:
                        success = self.add_document(file_path, doc_type="txt")
                    elif file_ext == '.json':
                        success = self.add_json_document(file_path)
                    elif file_ext in ['.docx', '.doc']:
                        success = self.add_word_document(file_path)
                    elif file_ext in ['.xlsx', '.xls']:
                        success = self.add_excel_document(file_path)
                    else:
                        print(f"è·³éä¸æ”¯æ´çš„æ–‡ä»¶é¡å‹: {file_path}")
                        continue
                    
                    if success:
                        successful_files += 1
                        
                except Exception as e:
                    print(f"è™•ç†æ–‡ä»¶ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        print(f"\nè¼‰å…¥å®Œæˆ: æˆåŠŸè™•ç† {successful_files}/{total_files} å€‹æ–‡ä»¶")
        return successful_files > 0
    
    def add_word_document(self, file_path: str, original_filename: str = None):
        """è™•ç†Wordæ–‡æª” (.docx, .doc)"""
        try:
            print(f"æ­£åœ¨è™•ç†Wordæ–‡æª”: {file_path}")
            
            doc = Document(file_path)
            
            full_text = []
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    full_text.append(paragraph.text.strip())
            
            for table in doc.tables:
                table_text = []
                for row in table.rows:
                    row_text = []
                    for cell in row.cells:
                        cell_text = cell.text.strip()
                        if cell_text:
                            row_text.append(cell_text)
                    if row_text:
                        table_text.append(" | ".join(row_text))
                
                if table_text:
                    full_text.append("è¡¨æ ¼å…§å®¹:\n" + "\n".join(table_text))
            
            text = "\n\n".join(full_text)
            
            if not text.strip():
                print(f"è­¦å‘Š: Wordæ–‡æª” {file_path} æ²’æœ‰æ–‡æœ¬å…§å®¹")
                return False
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=850,
                chunk_overlap=100,
                length_function=len,
            )
            chunks = text_splitter.split_text(text)
            
            ids = []
            embeddings = []
            documents = []
            metadatas = []
            
            for i, chunk in enumerate(chunks):
                response = ollama.embed(model=self.embedding_model, input=chunk)
                embedding = response["embeddings"][0]
                
                doc_id = f"{os.path.basename(file_path)}_{i}"
                ids.append(doc_id)
                embeddings.append(embedding)
                documents.append(chunk)
                metadatas.append({
                    "source": file_path, 
                    "chunk_id": i, 
                    "file_type": "docx",
                    "original_filename": original_filename if original_filename else os.path.basename(file_path),
                    "paragraphs_count": len(doc.paragraphs),
                    "tables_count": len(doc.tables)
                })
            
            self.collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )
            
            print(f"æˆåŠŸæ·»åŠ Wordæ–‡æª”: {file_path}ï¼Œå…±{len(chunks)}å€‹ç‰‡æ®µ")
            print(f"  - æ®µè½æ•¸: {len(doc.paragraphs)}")
            print(f"  - è¡¨æ ¼æ•¸: {len(doc.tables)}")
            return True
            
        except Exception as e:
            print(f"æ·»åŠ Wordæ–‡æª”å¤±æ•—: {e}")
            return False
    
    def add_excel_document(self, file_path: str, original_filename: str = None):
        """è™•ç†Excelæ–‡æª” (.xlsx, .xls)"""
        try:
            print(f"æ­£åœ¨è™•ç†Excelæ–‡æª”: {file_path}")
            
            try:
                excel_file = pd.ExcelFile(file_path)
                sheet_names = excel_file.sheet_names
                print(f"  - ç™¼ç¾ {len(sheet_names)} å€‹å·¥ä½œè¡¨: {sheet_names}")
                
                all_text = []
                
                for sheet_name in sheet_names:
                    print(f"  - è™•ç†å·¥ä½œè¡¨: {sheet_name}")
                    
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    
                    if df.empty:
                        print(f"    å·¥ä½œè¡¨ {sheet_name} ç‚ºç©ºï¼Œè·³é")
                        continue
                    
                    sheet_text = [f"å·¥ä½œè¡¨: {sheet_name}"]
                    sheet_text.append(f"è¡Œæ•¸: {len(df)}, åˆ—æ•¸: {len(df.columns)}")
                    sheet_text.append("åˆ—å: " + " | ".join(str(col) for col in df.columns))
                    sheet_text.append("")
                    
                    max_rows = min(1000, len(df))
                    for i in range(max_rows):
                        row_data = []
                        for col in df.columns:
                            cell_value = df.iloc[i][col]
                            if pd.isna(cell_value):
                                cell_value = ""
                            else:
                                cell_value = str(cell_value).strip()
                            row_data.append(cell_value)
                        
                        if any(row_data):
                            sheet_text.append(" | ".join(row_data))
                    
                    if len(df) > max_rows:
                        sheet_text.append(f"... (é‚„æœ‰ {len(df) - max_rows} è¡Œæ•¸æ“š)")
                    
                    all_text.append("\n".join(sheet_text))
                
                text = "\n\n" + "="*50 + "\n\n".join(all_text)
                
            except Exception as e:
                print(f"è®€å–Excelæ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                return False
            
            if not text.strip():
                print(f"è­¦å‘Š: Excelæ–‡æª” {file_path} æ²’æœ‰æ•¸æ“šå…§å®¹")
                return False
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1200,
                chunk_overlap=100,
                length_function=len,
            )
            chunks = text_splitter.split_text(text)
            
            ids = []
            embeddings = []
            documents = []
            metadatas = []
            
            for i, chunk in enumerate(chunks):
                response = ollama.embed(model=self.embedding_model, input=chunk)
                embedding = response["embeddings"][0]
                
                doc_id = f"{os.path.basename(file_path)}_{i}"
                ids.append(doc_id)
                embeddings.append(embedding)
                documents.append(chunk)
                metadatas.append({
                    "source": file_path, 
                    "chunk_id": i, 
                    "file_type": "excel",
                    "original_filename": original_filename if original_filename else os.path.basename(file_path),
                    "sheets_count": len(sheet_names),
                    "sheet_names": sheet_names
                })
            
            self.collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )
            
            print(f"æˆåŠŸæ·»åŠ Excelæ–‡æª”: {file_path}ï¼Œå…±{len(chunks)}å€‹ç‰‡æ®µ")
            print(f"  - å·¥ä½œè¡¨æ•¸: {len(sheet_names)}")
            return True
            
        except Exception as e:
            print(f"æ·»åŠ Excelæ–‡æª”å¤±æ•—: {e}")
            return False
    
    def add_json_document(self, file_path: str, original_filename: str = None):
        """å°ˆé–€è™•ç†JSONæ–‡æª”"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            if isinstance(json_data, dict):
                text = json.dumps(json_data, ensure_ascii=False, indent=2)
            elif isinstance(json_data, list):
                text = "\n".join([json.dumps(item, ensure_ascii=False) for item in json_data])
            else:
                text = str(json_data)
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=850,
                chunk_overlap=0,
                length_function=len,
            )
            chunks = text_splitter.split_text(text)
            
            ids = []
            embeddings = []
            documents = []
            metadatas = []
            
            for i, chunk in enumerate(chunks):
                response = ollama.embed(model=self.embedding_model, input=chunk)
                embedding = response["embeddings"][0]
                
                doc_id = f"{os.path.basename(file_path)}_{i}"
                ids.append(doc_id)
                embeddings.append(embedding)
                documents.append(chunk)
                metadatas.append({
                    "source": file_path, 
                    "chunk_id": i, 
                    "file_type": "json",
                    "original_filename": original_filename if original_filename else os.path.basename(file_path)
                })
            
            self.collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )
            
            print(f"æˆåŠŸæ·»åŠ JSONæ–‡æª”: {file_path}ï¼Œå…±{len(chunks)}å€‹ç‰‡æ®µ")
            return True
            
        except Exception as e:
            print(f"æ·»åŠ JSONæ–‡æª”å¤±æ•—: {e}")
            return False
    
    def add_document(self, file_path: str, doc_type: str = "txt", original_filename: str = None):
        """æ·»åŠ å–®å€‹æ–‡æª”åˆ°RAGè³‡æ–™åº«"""
        try:
            if doc_type.lower() == "pdf":
                loader = PyPDFLoader(file_path)
                documents = loader.load()
                text = "\n".join([doc.page_content for doc in documents])
            else:
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«æ™‚é–“æˆ³æ¨¡æ¿
            has_timestamp_template = "ç·¨è™Ÿèˆ‡æ—¥æœŸ:" in text
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=850,
                chunk_overlap=0,
                length_function=len,
            )
            chunks = text_splitter.split_text(text)
            
            ids = []
            embeddings = []
            documents = []
            metadatas = []
            
            for i, chunk in enumerate(chunks):
                response = ollama.embed(model=self.embedding_model, input=chunk)
                embedding = response["embeddings"][0]
                
                doc_id = f"{os.path.basename(file_path)}_{i}"
                ids.append(doc_id)
                embeddings.append(embedding)
                documents.append(chunk)
                metadatas.append({
                    "source": file_path, 
                    "chunk_id": i, 
                    "file_type": doc_type,
                    "original_filename": original_filename if original_filename else os.path.basename(file_path),
                    "has_timestamp_template": has_timestamp_template
                })
            
            self.collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )
            
            print(f"æˆåŠŸæ·»åŠ æ–‡æª”: {file_path}ï¼Œå…±{len(chunks)}å€‹ç‰‡æ®µ")
            if has_timestamp_template:
                print(f"  - åŒ…å«æ™‚é–“æˆ³æ¨¡æ¿")
            return True
            
        except Exception as e:
            print(f"æ·»åŠ æ–‡æª”å¤±æ•—: {e}")
            return False
    
    def list_loaded_documents(self):
        """é¡¯ç¤ºå·²è¼‰å…¥çš„æ–‡æª”åˆ—è¡¨"""
        try:
            all_docs = self.collection.get(include=["metadatas"])
            
            file_stats = {}
            for metadata in all_docs["metadatas"]:
                source = metadata.get("source", "unknown")
                filename = os.path.basename(source)
                file_type = metadata.get("file_type", "unknown")
                
                if filename not in file_stats:
                    file_stats[filename] = {
                        "chunks": 0,
                        "type": file_type,
                        "path": source,
                        "extra_info": {}
                    }
                file_stats[filename]["chunks"] += 1
                
                if file_type == "docx":
                    file_stats[filename]["extra_info"]["paragraphs"] = metadata.get("paragraphs_count", 0)
                    file_stats[filename]["extra_info"]["tables"] = metadata.get("tables_count", 0)
                elif file_type == "excel":
                    file_stats[filename]["extra_info"]["sheets"] = metadata.get("sheets_count", 0)
                    file_stats[filename]["extra_info"]["sheet_names"] = metadata.get("sheet_names", [])
            
            print(f"\nå·²è¼‰å…¥çš„æ–‡æª”åˆ—è¡¨ (å…± {len(file_stats)} å€‹æ–‡ä»¶):")
            for filename, stats in file_stats.items():
                extra_info = ""
                if stats["type"] == "docx" and stats["extra_info"]:
                    extra_info = f" [æ®µè½:{stats['extra_info'].get('paragraphs', 0)}, è¡¨æ ¼:{stats['extra_info'].get('tables', 0)}]";
                elif stats["type"] == "excel" and stats["extra_info"]:
                    sheets = stats['extra_info'].get('sheets', 0)
                    extra_info = f" [å·¥ä½œè¡¨:{sheets}]"
                
                icon = {
                    "txt": "ğŸ“„", "md": "ğŸ“", "pdf": "ğŸ“•", 
                    "json": "ğŸ“‹", "docx": "ğŸ“˜", "excel": "ğŸ“Š"
                }.get(stats["type"], "ğŸ“„")
                
                print(f"  {icon} {stats['display_name']} ({stats['type']}) - {stats['chunks']} å€‹ç‰‡æ®µ{extra_info}")
                
        except Exception as e:
            print(f"ç²å–æ–‡æª”åˆ—è¡¨å¤±æ•—: {e}")
    
    def search_documents(self, query: str, n_results: int = 6, date_range: str = '') -> List[Dict]:
        """æœç´¢æœ€ç›¸é—œçš„æ–‡æª”"""
        try:
            print(f"é–‹å§‹æœç´¢ï¼ŒæŸ¥è©¢: {query}, æ™‚é–“å€é–“: {date_range}")
            
            # æª¢æŸ¥åœæ­¢æ¨™èªŒ
            self._check_stop_flag()
            
            # ç²å–æŸ¥è©¢çš„åµŒå…¥å‘é‡
            response = ollama.embed(model=self.embedding_model, input=query)
            query_embedding = np.array(response["embeddings"][0]).reshape(1, -1)
            print(f"æŸ¥è©¢å‘é‡ç¶­åº¦: {query_embedding.shape}")
            
            # æª¢æŸ¥åœæ­¢æ¨™èªŒ
            self._check_stop_flag()
            
            # ç²å–æ‰€æœ‰æ–‡æª”
            all_docs = self.collection.get(include=["documents", "metadatas", "embeddings"])
            print(f"ç²å–åˆ° {len(all_docs['documents'])} å€‹æ–‡æª”")
            
            # èª¿è©¦ï¼šæª¢æŸ¥æ•¸æ“šçµæ§‹
            print("æ–‡æª”çµæ§‹:", {
                "documents": len(all_docs["documents"]),
                "metadatas": len(all_docs["metadatas"]),
                "embeddings": len(all_docs["embeddings"])
            })
            
            # å¦‚æœæœ‰æ™‚é–“å€é–“ï¼Œé€²è¡Œéæ¿¾
            if date_range and date_range.strip():
                try:
                    # æª¢æŸ¥æ˜¯å¦ç‚º "all time"
                    if date_range.strip() == "all time":
                        print("é¸æ“‡äº†æ‰€æœ‰æ™‚é–“ç¯„åœï¼Œä¸é€²è¡Œæ™‚é–“éæ¿¾")
                        filtered_docs = []
                        for i in range(len(all_docs["documents"])):
                            doc_content = all_docs["documents"][i]
                            doc_metadata = all_docs["metadatas"][i]
                            doc_embedding = all_docs["embeddings"][i]
                            doc_timestamp = None
                            
                            # å˜—è©¦å¾å…ƒæ•¸æ“šä¸­ç²å–æ™‚é–“æˆ³
                            if "timestamp" in doc_metadata:
                                try:
                                    doc_timestamp = int(doc_metadata["timestamp"])
                                except (ValueError, TypeError):
                                    pass
                            
                            # å¦‚æœå…ƒæ•¸æ“šä¸­æ²’æœ‰ï¼Œå‰‡æª¢æŸ¥æ–‡æª”å…§å®¹
                            if doc_timestamp is None and "ç·¨è™Ÿèˆ‡æ—¥æœŸ:" in doc_content:
                                import re
                                pattern = r'\(\d+\)\s*(\d{8})'
                                match = re.search(pattern, doc_content)
                                if match:
                                    doc_timestamp = int(match.group(1))
                            
                            filtered_docs.append({
                                "document": doc_content,
                                "metadata": doc_metadata,
                                "embedding": doc_embedding,
                                "timestamp": str(doc_timestamp) if doc_timestamp else ""
                            })
                    else:
                        start_date, end_date = date_range.split(' - ')
                        start_date = int(start_date.strip())
                        end_date = int(end_date.strip())
                        print(f"æ™‚é–“å€é–“: {start_date} - {end_date}")
                        
                        filtered_docs = []
                        for i in range(len(all_docs["documents"])):
                            # å¾æ–‡æª”å…§å®¹ä¸­æå–æ™‚é–“æˆ³
                            doc_content = all_docs["documents"][i]
                            doc_metadata = all_docs["metadatas"][i]
                            doc_embedding = all_docs["embeddings"][i]
                            doc_timestamp = None
                            
                            # é¦–å…ˆæª¢æŸ¥å…ƒæ•¸æ“šä¸­æ˜¯å¦æœ‰æ™‚é–“æˆ³
                            if "timestamp" in doc_metadata:
                                try:
                                    doc_timestamp = int(doc_metadata["timestamp"])
                                except (ValueError, TypeError) as e:
                                    print(f"è§£ææ™‚é–“æˆ³å¤±æ•—: {e}, åŸå§‹å€¼: {doc_metadata['timestamp']}")
                                    pass
                            
                            # å¦‚æœå…ƒæ•¸æ“šä¸­æ²’æœ‰ï¼Œå‰‡æª¢æŸ¥æ–‡æª”å…§å®¹
                            if doc_timestamp is None and "ç·¨è™Ÿèˆ‡æ—¥æœŸ:" in doc_content:
                                import re
                                pattern = r'\(\d+\)\s*(\d{8})'
                                match = re.search(pattern, doc_content)
                                if match:
                                    doc_timestamp = int(match.group(1))
                                    print(f"å¾å…§å®¹ä¸­æ‰¾åˆ°æ™‚é–“æˆ³: {doc_timestamp}")
                            
                            # å¦‚æœé‚„æ˜¯æ²’æœ‰æ‰¾åˆ°æ™‚é–“æˆ³ï¼Œè·³éæ­¤æ–‡æª”
                            if doc_timestamp is None:
                                print(f"æ–‡æª”æ²’æœ‰æ™‚é–“æˆ³ï¼Œè·³é: {doc_metadata.get('original_filename', 'unknown')}")
                                continue
                            
                            # æª¢æŸ¥æ™‚é–“æˆ³æ˜¯å¦åœ¨æŒ‡å®šç¯„åœå…§
                            if start_date <= doc_timestamp <= end_date:
                                filtered_docs.append({
                                    "document": doc_content,
                                    "metadata": doc_metadata,
                                    "embedding": doc_embedding,
                                    "timestamp": str(doc_timestamp)
                                })
                        
                        print(f"æ™‚é–“å€é–“éæ¿¾å¾Œå‰©é¤˜ {len(filtered_docs)} å€‹æ–‡æª”")
                        
                        if len(filtered_docs) == 0:
                            print("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ™‚é–“å€é–“çš„æ–‡æª”")
                            return []
                            
                except Exception as e:
                    print(f"æ™‚é–“å€é–“éæ¿¾å¤±æ•—: {str(e)}")
                    # å¦‚æœæ™‚é–“éæ¿¾å¤±æ•—ï¼Œå‰‡å›é€€åˆ°ä¸éæ¿¾ï¼Œä½†ä»å˜—è©¦æå–æ™‚é–“æˆ³
                    filtered_docs = []
                    for i in range(len(all_docs["documents"])):
                        doc_content = all_docs["documents"][i]
                        doc_metadata = all_docs["metadatas"][i]
                        doc_embedding = all_docs["embeddings"][i]
                        doc_timestamp = None
                        if "timestamp" in doc_metadata:
                            try:
                                doc_timestamp = int(doc_metadata["timestamp"])
                            except (ValueError, TypeError):
                                pass
                        if doc_timestamp is None and "ç·¨è™Ÿèˆ‡æ—¥æœŸ:" in doc_content:
                            import re
                            pattern = r'\(\d+\)\s*(\d{8})'
                            match = re.search(pattern, doc_content)
                            if match:
                                doc_timestamp = int(match.group(1))
                        
                        filtered_docs.append({
                            "document": doc_content,
                            "metadata": doc_metadata,
                            "embedding": doc_embedding,
                            "timestamp": str(doc_timestamp) if doc_timestamp else ""
                        })
            else:
                # å¦‚æœæ²’æœ‰æ™‚é–“å€é–“ï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰æ–‡æª”ï¼Œä½†ä»ç„¶æå–æ™‚é–“æˆ³
                filtered_docs = []
                for i in range(len(all_docs["documents"])):
                    doc_content = all_docs["documents"][i]
                    doc_metadata = all_docs["metadatas"][i]
                    doc_embedding = all_docs["embeddings"][i]
                    doc_timestamp = None
                    
                    # å˜—è©¦å¾å…ƒæ•¸æ“šä¸­ç²å–æ™‚é–“æˆ³
                    if "timestamp" in doc_metadata:
                        try:
                            doc_timestamp = int(doc_metadata["timestamp"])
                        except (ValueError, TypeError):
                            pass
                    
                    # å¦‚æœå…ƒæ•¸æ“šä¸­æ²’æœ‰ï¼Œå‰‡æª¢æŸ¥æ–‡æª”å…§å®¹
                    if doc_timestamp is None and "ç·¨è™Ÿèˆ‡æ—¥æœŸ:" in doc_content:
                        import re
                        pattern = r'\(\d+\)\s*(\d{8})'
                        match = re.search(pattern, doc_content)
                        if match:
                            doc_timestamp = int(match.group(1))
                    
                    filtered_docs.append({
                        "document": doc_content,
                        "metadata": doc_metadata,
                        "embedding": doc_embedding,
                        "timestamp": str(doc_timestamp) if doc_timestamp else "" # ç¢ºä¿æœ‰timestampå­—æ®µ
                    })
            
            # è¨ˆç®—ç›¸ä¼¼åº¦
            results = []
            for doc in filtered_docs:
                try:
                    # ç¢ºä¿å‘é‡ç¶­åº¦æ­£ç¢º
                    doc_vec = np.array(doc["embedding"]).reshape(1, -1)
                    if doc_vec.shape[1] != query_embedding.shape[1]:
                        print(f"å‘é‡ç¶­åº¦ä¸åŒ¹é…: æ–‡æª”å‘é‡ {doc_vec.shape}, æŸ¥è©¢å‘é‡ {query_embedding.shape}")
                        continue
                        
                    similarity = cosine_similarity(query_embedding, doc_vec)[0][0]
                    distance = 1 - similarity
                    results.append({
                        "content": doc["document"],
                        "metadata": doc["metadata"],
                        "distance": distance,
                        "chunk_id": doc["metadata"].get("chunk_id", 0),
                        "timestamp": doc.get("timestamp", "")
                    })
                except Exception as e:
                    print(f"è¨ˆç®—ç›¸ä¼¼åº¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    continue
            
            # æŒ‰è·é›¢æ’åºä¸¦è¿”å›å‰ n_results å€‹çµæœ
            results.sort(key=lambda x: x["distance"])
            final_results = results[:n_results]
            print(f"è¿”å› {len(final_results)} å€‹çµæœ")
            
            # åœ¨é—œéµæ­¥é©Ÿå¾Œæª¢æŸ¥åœæ­¢æ¨™èªŒ
            self._check_stop_flag()
            
            return final_results
            
        except Exception as e:
            if self._stop_flag:
                print("æœç´¢ä»»å‹™è¢«ç”¨æˆ¶ä¸­æ–·")
                return []
            print(f"æ–‡æª”æœç´¢å¤±æ•—: {str(e)}")
            return []
    
    def filter_documents(self, query: str, documents: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨ç¬¬ä¸€å€‹LLMç¯©é¸æ–‡æª”"""
        relevant_docs = []
        filter_interactions = {}  # ä¿å­˜æ¯å€‹æ–‡æª”çš„äº¤äº’è¨Šæ¯
        
        for doc in documents:
            # æª¢æŸ¥åœæ­¢æ¨™èªŒ
            self._check_stop_flag()
            
            filter_user_proxy = autogen.UserProxyAgent(
                name="filter_user_proxy",
                human_input_mode="NEVER",
                max_consecutive_auto_reply=1,
                is_termination_msg=lambda x: True,
                code_execution_config=False,
                llm_config=self.llm_config,
            )
            
            filter_prompt = f"""
ç”¨æˆ¶å•é¡Œ: {query}

æ–‡æª”å…§å®¹:
{doc['content']}

è«‹åˆ¤æ–·æ­¤æ–‡æª”æ˜¯å¦èˆ‡ç”¨æˆ¶å•é¡Œç›¸é—œã€‚
"""
            
            # è¨˜éŒ„äº¤äº’é–‹å§‹
            interaction_messages = []
            interaction_messages.append({
                'role': 'user',
                'content': filter_prompt,
                'timestamp': time.time()
            })
            
            filter_user_proxy.initiate_chat(
                self.document_filter,
                message=filter_prompt,
                max_turns=1
            )
            
            try:
                last_message = filter_user_proxy.last_message(self.document_filter)
                if last_message:
                    # å°‡AIåˆ†æçµæœè½‰æ›ç‚ºç¹é«”ä¸­æ–‡
                    traditional_content = convert_to_traditional(last_message["content"])
                    
                    interaction_messages.append({
                        'role': 'assistant',
                        'content': traditional_content,
                        'timestamp': time.time()
                    })
                    
                    # ä¿å­˜äº¤äº’è¨Šæ¯
                    doc_key = f"{doc['metadata'].get('original_filename', os.path.basename(doc['metadata']['source']))}_{doc['metadata'].get('chunk_id', 0)}"
                    filter_interactions[doc_key] = {
                        'messages': interaction_messages,
                        'is_relevant': "NOT_RELEVANT:" not in traditional_content,
                        'filename': doc['metadata'].get('original_filename', os.path.basename(doc['metadata']['source'])),
                        'chunk_id': doc['metadata'].get('chunk_id', 0)
                    }
                    
                    if "NOT_RELEVANT:" not in traditional_content:
                        relevant_docs.append(doc)
            except Exception as e:
                print(f"ç²å–ç¯©é¸çµæœå¤±æ•—: {e}")
                # å³ä½¿å¤±æ•—ä¹Ÿä¿å­˜äº¤äº’è¨Šæ¯
                doc_key = f"{doc['metadata'].get('original_filename', os.path.basename(doc['metadata']['source']))}_{doc['metadata'].get('chunk_id', 0)}"
                filter_interactions[doc_key] = {
                    'messages': interaction_messages,
                    'is_relevant': True,  # å¤±æ•—æ™‚é»˜èªèªç‚ºç›¸é—œ
                    'filename': doc['metadata'].get('original_filename', os.path.basename(doc['metadata']['source'])),
                    'chunk_id': doc['metadata'].get('chunk_id', 0),
                    'error': str(e)
                }
                relevant_docs.append(doc)
            
            # æ¯è™•ç†å®Œä¸€å€‹æ–‡æª”å¾Œä¼‘æ¯3ç§’
            time.sleep(12)
            
            # æª¢æŸ¥åœæ­¢æ¨™èªŒ
            self._check_stop_flag()
        
        # å°‡äº¤äº’è¨Šæ¯æ·»åŠ åˆ°æ¯å€‹ç›¸é—œæ–‡æª”ä¸­
        for doc in relevant_docs:
            doc_key = f"{doc['metadata'].get('original_filename', os.path.basename(doc['metadata']['source']))}_{doc['metadata'].get('chunk_id', 0)}"
            if doc_key in filter_interactions:
                doc['filter_interaction'] = filter_interactions[doc_key]
        
        return relevant_docs, filter_interactions
    
    def generate_answer(self, query: str, relevant_docs: List[Dict]) -> str:
        """ä½¿ç”¨ç¬¬äºŒå€‹LLMç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ"""
        if len(relevant_docs) == 0:
            return "æ²’æœ‰æ‰¾åˆ°ç›¸é—œæ–‡æª”ä¾†å›ç­”æ‚¨çš„å•é¡Œã€‚"
        
        # æª¢æŸ¥åœæ­¢æ¨™èªŒ
        self._check_stop_flag()
        
        synthesis_user_proxy = autogen.UserProxyAgent(
            name="synthesis_user_proxy",
            human_input_mode="NEVER",
            max_consecutive_auto_reply=1,
            is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
            code_execution_config=False,
            llm_config=self.llm_config,
        )
        
        context = "\n\n".join([f"æ–‡æª”{i+1}:\n{doc['content']}" 
                              for i, doc in enumerate(relevant_docs)])
        
        synthesis_prompt = f"""
åŸºæ–¼ä»¥ä¸‹ç›¸é—œæ–‡æª”å›ç­”ç”¨æˆ¶å•é¡Œ:

ç”¨æˆ¶å•é¡Œ: {query}

ç›¸é—œæ–‡æª”:
{context}

è«‹æä¾›ç¶œåˆæ€§çš„ç­”æ¡ˆ:
"""
        
        synthesis_user_proxy.initiate_chat(
            self.answer_synthesizer,
            message=synthesis_prompt,
            max_turns=1
        )
        
        try:
            # æª¢æŸ¥åœæ­¢æ¨™èªŒ
            self._check_stop_flag()
            
            last_message = synthesis_user_proxy.last_message(self.answer_synthesizer)
            if last_message:
                return convert_to_traditional(last_message["content"])
                # return last_message["content"]
            return "ç”Ÿæˆç­”æ¡ˆå¤±æ•—"
        except Exception as e:
            if self._stop_flag:
                return "ç­”æ¡ˆç”Ÿæˆè¢«ç”¨æˆ¶ä¸­æ–·"
            print(f"ç²å–ç­”æ¡ˆå¤±æ•—: {e}")
            return "ç”Ÿæˆç­”æ¡ˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤"
    
    def query(self, question: str) -> str:
        """å®Œæ•´çš„RAGæŸ¥è©¢æµç¨‹"""
        print(f"\né–‹å§‹è™•ç†å•é¡Œ: {question}")
        
        print("1. æœç´¢ç›¸é—œæ–‡æª”...")
        documents = self.search_documents(question, n_results=6)
        print(f"æ‰¾åˆ° {len(documents)} å€‹å€™é¸æ–‡æª”")
        
        if not documents:
            return "æ²’æœ‰æ‰¾åˆ°ç›¸é—œæ–‡æª”"
        
        print("æ‰¾åˆ°çš„æ–‡æª”:")
        for i, doc in enumerate(documents, 1):
            filename = os.path.basename(doc['metadata']['source'])
            file_type = doc['metadata'].get('file_type', 'unknown')
            distance = doc['distance']
            print(f"  {i}. {filename} ({file_type}) - ç›¸ä¼¼åº¦: {1-distance:.3f}")
        
        print("\n2. ç¯©é¸ç›¸é—œæ–‡æª”...")
        relevant_docs, filter_interactions = self.filter_documents(question, documents)
        print(f"ç¯©é¸å¾Œä¿ç•™ {len(relevant_docs)} å€‹ç›¸é—œæ–‡æª”")
        
        if relevant_docs:
            print("ä¿ç•™çš„æ–‡æª”:")
            for i, doc in enumerate(relevant_docs, 1):
                filename = os.path.basename(doc['metadata']['source'])
                file_type = doc['metadata'].get('file_type', 'unknown')
                print(f"  {i}. {filename} ({file_type})")
        
        print("\n3. ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ...")
        answer = self.generate_answer(question, relevant_docs)
        
        return answer 